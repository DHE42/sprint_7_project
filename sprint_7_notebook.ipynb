{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414eedc0",
   "metadata": {},
   "source": [
    "## Megaline Legacy Plan Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462974a",
   "metadata": {},
   "source": [
    "I'll begin by importing the libraries and modules for handling data, building and evaluating regression and classification models, splitting datasets, and preprocessing features. Then, I'll load the DataFrame I'll be using from the Github repository I have set up for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a4a09d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "\n",
      "             calls      minutes     messages       mb_used     is_ultra\n",
      "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
      "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
      "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
      "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
      "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
      "max     244.000000  1632.060000   224.000000  49745.730000     1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/DHE42/sprint_7_project/refs/heads/main/users_behavior.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "print(df.info())\n",
    "print()\n",
    "\n",
    "print(df.describe())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25230bb",
   "metadata": {},
   "source": [
    "It is evident from preliminary explorations that there are five columns, and that is_ultra is the target. Since we do not have separate training, validation, and testing datasets, I will split the data into 60% and 40%, and then divide that 40% in half to receive a training data set of 60%, a validation data set of 20%, and a testing dataset of 20%. I will also declare all columns but is_ultra the features, and is_ultra the target. Finally, I will write a function that evaluates the models I will need to train in order to gauge their accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eb86be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare features and target\n",
    "features = df.drop(['is_ultra'], axis=1)\n",
    "target = df['is_ultra']\n",
    "\n",
    "\n",
    "# Split of a 20% test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345\n",
    ")\n",
    "\n",
    "# Split features_train and target_train into training (60%) and validation (20%)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=12345\n",
    ")\n",
    "\n",
    "def evaluate_model(model, features_valid, target_valid, features_test, target_test, model_name=\"Model\"):\n",
    "   \n",
    "    # Validation set predictions and accuracy\n",
    "    valid_predictions = model.predict(features_valid)\n",
    "    valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "    print(f\"{model_name} Validation Accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    # Test set predictions and accuracy\n",
    "    test_predictions = model.predict(features_test)\n",
    "    test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Error count\n",
    "    validation_errors = sum(target_valid != valid_predictions)\n",
    "    test_errors = sum(target_test != test_predictions)\n",
    "    print(f\"{model_name} Validation Errors: {validation_errors}\")\n",
    "    print(f\"{model_name} Test Errors: {test_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1ff63",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef67866",
   "metadata": {},
   "source": [
    "I will begin my model exploration by writing a function to explore several different decision tree hyperparameters and give me the hyperparameters that will result in the highest accuracy scores. Decision trees tend to be very fast with the drawback of underfittedness if the depth of the tree is under four, with the converse problem if it is over four. Let's play around with some other hyperparameters: min_samples_split, criterion, and gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbace234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Performing Configuration:\n",
      "{'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'valid_accuracy': 0.7667185069984448, 'test_accuracy': 0.7962674961119751}\n",
      "\n",
      "Decision Tree Validation Accuracy: 0.7667185069984448\n",
      "Decision Tree Test Accuracy: 0.7962674961119751\n",
      "Decision Tree Validation Errors: 150\n",
      "Decision Tree Test Errors: 131\n"
     ]
    }
   ],
   "source": [
    "# Write function to explore decision tree hyperparameters and evaluate their performance\n",
    "\n",
    "def explore_decision_tree_hyperparameters(features_train, target_train, features_valid, target_valid, features_test, target_test):\n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Depth variations\n",
    "        {'max_depth': 3, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        \n",
    "        # Splitting variations\n",
    "        {'max_depth': 5, 'min_samples_split': 5, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_split': 10, 'criterion': 'gini'},\n",
    "        \n",
    "        # Criterion variations\n",
    "        {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy'},\n",
    "        \n",
    "        # Leaf node variations\n",
    "        {'max_depth': 5, 'min_samples_leaf': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through configurations\n",
    "    for config in configs:\n",
    "        # Train Decision Tree\n",
    "        tree_model = DecisionTreeClassifier(\n",
    "            random_state=12345,\n",
    "            **config\n",
    "        )\n",
    "        tree_model.fit(features_train, target_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_predictions = tree_model.predict(features_valid)\n",
    "        test_predictions = tree_model.predict(features_test)\n",
    "        \n",
    "        valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "        test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "        \n",
    "        # Store results\n",
    "        result = config.copy()\n",
    "        result['valid_accuracy'] = valid_accuracy\n",
    "        result['test_accuracy'] = test_accuracy\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by test accuracy\n",
    "    results_sorted = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop Performing Configuration:\")\n",
    "    print(results_sorted[0])\n",
    "    print()\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "hyperparameter_results = explore_decision_tree_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")\n",
    "\n",
    "# Take the best configuration\n",
    "best_config = hyperparameter_results[0]\n",
    "\n",
    "# Declare and fit final Decision Tree\n",
    "tree_model = DecisionTreeClassifier(\n",
    "    random_state=12345,\n",
    "    **{k: v for k, v in best_config.items() if k not in ['valid_accuracy', 'test_accuracy']}\n",
    ")\n",
    "tree_model.fit(features_train, target_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "evaluate_model(tree_model, features_valid, target_valid, features_test, target_test, model_name=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6e0bd",
   "metadata": {},
   "source": [
    "The function I wrote cycled through several iterations of Decision Tree hyperparameters, giving a final top performing configuration and accuracy scores for both the validation set and test set. The test set scored about 80% accuracy, which meets the goal of at least 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d402c67",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11223a73",
   "metadata": {},
   "source": [
    "Let's do something similar with a random forest. These tend to have the highest accuracy since it uses a tree ensemble, but the slowest speed due to the large amount of decisions it cycles through. We'll try different configurations for n_estimators, max_depth, min_samples_split, criterion, and gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5d63e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Performing Configuration:\n",
      "{'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini', 'valid_accuracy': 0.7791601866251944, 'test_accuracy': 0.7931570762052877}\n",
      "\n",
      "Random Forest Validation Accuracy: 0.7791601866251944\n",
      "Random Forest Test Accuracy: 0.7931570762052877\n",
      "Random Forest Validation Errors: 142\n",
      "Random Forest Test Errors: 133\n"
     ]
    }
   ],
   "source": [
    "# Write function to explore random forest hyperparameters and evaluate their performance\n",
    "def explore_random_forest_hyperparameters(features_train, target_train, features_valid, target_valid, features_test, target_test):\n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 200, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 7, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 10, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'max_features': 'log2'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'bootstrap': False},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'bootstrap': True}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Try all configurations\n",
    "    for config in configs:\n",
    "        model = RandomForestClassifier(random_state=12345, **config)\n",
    "        model.fit(features_train, target_train)\n",
    "        \n",
    "        valid_predictions = model.predict(features_valid)\n",
    "        test_predictions = model.predict(features_test)\n",
    "        \n",
    "        valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "        test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "        \n",
    "        result = config.copy()\n",
    "        result['valid_accuracy'] = valid_accuracy\n",
    "        result['test_accuracy'] = test_accuracy\n",
    "        results.append(result)\n",
    "    \n",
    "    # Pick best config (highest accuracy score for test set)\n",
    "    best_config = max(results, key=lambda x: x['test_accuracy'])\n",
    "    \n",
    "    print(\"\\nTop Performing Configuration:\")\n",
    "    print(best_config)\n",
    "    print()\n",
    "    \n",
    "    # Build and fit final model with best hyperparameters\n",
    "    forest_model = RandomForestClassifier(\n",
    "        random_state=12345,\n",
    "        **{k: v for k, v in best_config.items() if k not in ['valid_accuracy','test_accuracy']}\n",
    "    )\n",
    "    forest_model.fit(features_train, target_train)\n",
    "    \n",
    "    # Evaluate final model\n",
    "    evaluate_model(forest_model, features_valid, target_valid, features_test, target_test, model_name=\"Random Forest\")\n",
    "    \n",
    "    return forest_model, results\n",
    "\n",
    "\n",
    "# Call the function\n",
    "forest_model, results = explore_random_forest_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562bd46",
   "metadata": {},
   "source": [
    "Interesting! It looks like the random forest model had lower accuracy than the decision tree model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc8562",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e95ef0",
   "metadata": {},
   "source": [
    "Logistic regression models typically have medium accuracy, but high speed like decision tree models. It has the lowest number of hyperparameters, funnily enough. We'll play around with the the solver hyperparameter mostly, with the max_iter hyperparameter set at 5000 to avoid errors when computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98573f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Performing Configuration:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0, 'max_iter': 5000, 'valid_accuracy': 0.7278382581648523, 'test_accuracy': 0.7589424572317263}\n",
      "\n",
      "Logistic Regression Validation Accuracy: 0.7278382581648523\n",
      "Logistic Regression Test Accuracy: 0.7589424572317263\n",
      "Logistic Regression Validation Errors: 175\n",
      "Logistic Regression Test Errors: 155\n"
     ]
    }
   ],
   "source": [
    "def explore_logistic_regression_hyperparameters(features_train, target_train,\n",
    "                                                 features_valid, target_valid,\n",
    "                                                 features_test, target_test):\n",
    "    \n",
    "    # Scaled features to prevent logistic regression failure\n",
    "    scaler = StandardScaler()\n",
    "    features_train_scaled = scaler.fit_transform(features_train)\n",
    "    features_valid_scaled = scaler.transform(features_valid)\n",
    "    features_test_scaled = scaler.transform(features_test)\n",
    "    \n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Solvers with default penalty options\n",
    "        {'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0, 'max_iter': 5000},\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0, 'max_iter': 5000},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 1.0, 'max_iter': 5000},\n",
    "        {'solver': 'saga', 'penalty': 'l1', 'C': 1.0, 'max_iter': 5000},\n",
    "        {'solver': 'saga', 'penalty': 'l2', 'C': 1.0, 'max_iter': 5000},\n",
    "        # Regularization strength variations\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.1, 'max_iter': 5000},\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 10, 'max_iter': 5000},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1, 'max_iter': 5000},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10, 'max_iter': 5000},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through configurations\n",
    "    for config in configs:\n",
    "        logistic_model = LogisticRegression(random_state=12345, **config)\n",
    "        logistic_model.fit(features_train_scaled, target_train)\n",
    "            \n",
    "        # Evaluate\n",
    "        valid_predictions = logistic_model.predict(features_valid_scaled)\n",
    "        test_predictions = logistic_model.predict(features_test_scaled)\n",
    "            \n",
    "        valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "        test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "            \n",
    "        # Store results\n",
    "        result = config.copy()\n",
    "        result['valid_accuracy'] = valid_accuracy\n",
    "        result['test_accuracy'] = test_accuracy\n",
    "        results.append(result)\n",
    "\n",
    "    \n",
    "    # Sort results by test accuracy\n",
    "    results_sorted = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "    \n",
    "    # Pick best config\n",
    "    best_config = results_sorted[0]\n",
    "    print(\"Top Performing Configuration:\")\n",
    "    print(best_config)\n",
    "    print()\n",
    "    \n",
    "    # Build and fit final model with best hyperparameters\n",
    "    final_logistic_model = LogisticRegression(\n",
    "        random_state=12345,\n",
    "        **{k: v for k, v in best_config.items() if k not in ['valid_accuracy', 'test_accuracy']}\n",
    "    )\n",
    "    final_logistic_model.fit(features_train_scaled, target_train)\n",
    "    \n",
    "    # Evaluate final model\n",
    "    evaluate_model(final_logistic_model, features_valid_scaled, target_valid,\n",
    "                   features_test_scaled, target_test, model_name=\"Logistic Regression\")\n",
    "    \n",
    "    return final_logistic_model, results\n",
    "\n",
    "logistic_model, results = explore_logistic_regression_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c0c84c",
   "metadata": {},
   "source": [
    "In practice, it appears that this model has the lowest accuracy score. You learn something new every day!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bd9d7",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f580361",
   "metadata": {},
   "source": [
    "In practice, it appears that thigns bore out rather differently than in the classroom. It looks like the decistion tree had highest accuracy, followed by random forest, followed by logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
