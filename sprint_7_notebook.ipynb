{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a58b75a",
   "metadata": {},
   "source": [
    "Project instructions\n",
    "\n",
    "1) Open and look through the data file. Path to the file:datasets/users_behavior.csv Download dataset\n",
    "2) Split the source data into a training set, a validation set, and a test set.\n",
    "3) Investigate the quality of different models by changing hyperparameters. Briefly describe the findings of the study.\n",
    "4) Check the quality of the model using the test set.\n",
    "5) Additional task: sanity check the model. This data is more complex than what you’re used to working with, so it's not an easy task. We'll take a closer look at it later.\n",
    "\n",
    "Here’s what the reviewers will look at when reviewing your project:\n",
    "\n",
    "1) How did you look into data after downloading?\n",
    "2) Have you correctly split the data into train, validation, and test sets?\n",
    "3) How have you chosen the sets' sizes?\n",
    "4) Did you evaluate the quality of the models correctly?\n",
    "5) What models and hyperparameters did you use?\n",
    "6) What are your findings?\n",
    "7) Did you test the models correctly?\n",
    "8) What is your accuracy score?\n",
    "9) Have you stuck to the project structure and kept the code neat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4a09d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   calls  minutes  messages   mb_used  is_ultra\n",
      "0   40.0   311.90      83.0  19915.42         0\n",
      "1   85.0   516.75      56.0  22696.96         0\n",
      "2   77.0   467.66      86.0  21060.45         0\n",
      "3  106.0   745.53      81.0   8437.39         1\n",
      "4   66.0   418.74       1.0  14502.75         0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   calls     3214 non-null   float64\n",
      " 1   minutes   3214 non-null   float64\n",
      " 2   messages  3214 non-null   float64\n",
      " 3   mb_used   3214 non-null   float64\n",
      " 4   is_ultra  3214 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n",
      "None\n",
      "\n",
      "             calls      minutes     messages       mb_used     is_ultra\n",
      "count  3214.000000  3214.000000  3214.000000   3214.000000  3214.000000\n",
      "mean     63.038892   438.208787    38.281269  17207.673836     0.306472\n",
      "std      33.236368   234.569872    36.148326   7570.968246     0.461100\n",
      "min       0.000000     0.000000     0.000000      0.000000     0.000000\n",
      "25%      40.000000   274.575000     9.000000  12491.902500     0.000000\n",
      "50%      62.000000   430.600000    30.000000  16943.235000     0.000000\n",
      "75%      82.000000   571.927500    57.000000  21424.700000     1.000000\n",
      "max     244.000000  1632.060000   224.000000  49745.730000     1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/DHE42/sprint_7_project/refs/heads/main/users_behavior.csv'\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())\n",
    "print()\n",
    "\n",
    "print(df.info())\n",
    "print()\n",
    "\n",
    "print(df.describe())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eb86be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare features and target\n",
    "features = df.drop(['is_ultra'], axis=1)\n",
    "target = df['is_ultra']\n",
    "\n",
    "\n",
    "# Split of a 20% test set\n",
    "features_train, features_test, target_train, target_test = train_test_split(\n",
    "    features, target, test_size=0.2, random_state=12345\n",
    ")\n",
    "\n",
    "# Split features_train and target_train into training (60%) and validation (20%)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train, target_train, test_size=0.25, random_state=12345\n",
    ")\n",
    "\n",
    "def evaluate_model(model, features_valid, target_valid, features_test, target_test, model_name=\"Model\"):\n",
    "   \n",
    "    # Validation set predictions and accuracy\n",
    "    valid_predictions = model.predict(features_valid)\n",
    "    valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "    print(f\"{model_name} Validation Accuracy: {valid_accuracy}\")\n",
    "    \n",
    "    # Test set predictions and accuracy\n",
    "    test_predictions = model.predict(features_test)\n",
    "    test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "    print(f\"{model_name} Test Accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Error count\n",
    "    validation_errors = sum(target_valid != valid_predictions)\n",
    "    test_errors = sum(target_test != test_predictions)\n",
    "    print(f\"{model_name} Validation Errors: {validation_errors}\")\n",
    "    print(f\"{model_name} Test Errors: {test_errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c1ff63",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cbace234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Performing Configuration:\n",
      "{'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy', 'valid_accuracy': 0.7667185069984448, 'test_accuracy': 0.7962674961119751}\n",
      "\n",
      "Decision Tree Validation Accuracy: 0.7667185069984448\n",
      "Decision Tree Test Accuracy: 0.7962674961119751\n",
      "Decision Tree Validation Errors: 150\n",
      "Decision Tree Test Errors: 131\n"
     ]
    }
   ],
   "source": [
    "# Write function to explore decision tree hyperparameters and evaluate their performance\n",
    "\n",
    "def explore_decision_tree_hyperparameters(features_train, target_train, features_valid, target_valid, features_test, target_test):\n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Depth variations\n",
    "        {'max_depth': 3, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        \n",
    "        # Splitting variations\n",
    "        {'max_depth': 5, 'min_samples_split': 5, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_split': 10, 'criterion': 'gini'},\n",
    "        \n",
    "        # Criterion variations\n",
    "        {'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy'},\n",
    "        \n",
    "        # Leaf node variations\n",
    "        {'max_depth': 5, 'min_samples_leaf': 2, 'criterion': 'gini'},\n",
    "        {'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through configurations\n",
    "    for config in configs:\n",
    "        # Train Decision Tree\n",
    "        tree_model = DecisionTreeClassifier(\n",
    "            random_state=12345,\n",
    "            **config\n",
    "        )\n",
    "        tree_model.fit(features_train, target_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        valid_predictions = tree_model.predict(features_valid)\n",
    "        test_predictions = tree_model.predict(features_test)\n",
    "        \n",
    "        valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "        test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "        \n",
    "        # Store results\n",
    "        result = config.copy()\n",
    "        result['valid_accuracy'] = valid_accuracy\n",
    "        result['test_accuracy'] = test_accuracy\n",
    "        results.append(result)\n",
    "    \n",
    "    # Sort by test accuracy\n",
    "    results_sorted = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop Performing Configuration:\")\n",
    "    print(results_sorted[0])\n",
    "    print()\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "hyperparameter_results = explore_decision_tree_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")\n",
    "\n",
    "# Take the best configuration\n",
    "best_config = hyperparameter_results[0]\n",
    "\n",
    "# Declare and fit final Decision Tree\n",
    "tree_model = DecisionTreeClassifier(\n",
    "    random_state=12345,\n",
    "    **{k: v for k, v in best_config.items() if k not in ['valid_accuracy', 'test_accuracy']}\n",
    ")\n",
    "tree_model.fit(features_train, target_train)\n",
    "\n",
    "# Evaluate the final model\n",
    "evaluate_model(tree_model, features_valid, target_valid, features_test, target_test, model_name=\"Decision Tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6e0bd",
   "metadata": {},
   "source": [
    "The function I wrote cycled through several iterations of Decision Tree hyperparameters, giving a final top performing configuration and accuracy scores for both the validation set and test set. The test set scored about 80% accuracy, which meets the goal of at least 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d402c67",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5d63e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Performing Configuration:\n",
      "{'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini', 'valid_accuracy': 0.7791601866251944, 'test_accuracy': 0.7931570762052877}\n",
      "\n",
      "Random Forest Validation Accuracy: 0.7791601866251944\n",
      "Random Forest Test Accuracy: 0.7931570762052877\n",
      "Random Forest Validation Errors: 142\n",
      "Random Forest Test Errors: 133\n"
     ]
    }
   ],
   "source": [
    "# Write function to explore random forest hyperparameters and evaluate their performance\n",
    "\n",
    "def explore_random_forest_hyperparameters(features_train, target_train, features_valid, target_valid, features_test, target_test):\n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 200, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 7, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 5, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 10, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'entropy'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 2, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_leaf': 4, 'criterion': 'gini'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'max_features': 'sqrt'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'max_features': 'log2'},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'bootstrap': False},\n",
    "        {'n_estimators': 100, 'max_depth': 5, 'min_samples_split': 2, 'bootstrap': True}\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Try all configurations\n",
    "    for config in configs:\n",
    "        model = RandomForestClassifier(random_state=12345, **config)\n",
    "        model.fit(features_train, target_train)\n",
    "        \n",
    "        valid_predictions = model.predict(features_valid)\n",
    "        test_predictions = model.predict(features_test)\n",
    "        \n",
    "        valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "        test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "        \n",
    "        result = config.copy()\n",
    "        result['valid_accuracy'] = valid_accuracy\n",
    "        result['test_accuracy'] = test_accuracy\n",
    "        results.append(result)\n",
    "    \n",
    "    # Pick best config (highest accuracy score for test set)\n",
    "    best_config = max(results, key=lambda x: x['test_accuracy'])\n",
    "    \n",
    "    print(\"\\nTop Performing Configuration:\")\n",
    "    print(best_config)\n",
    "    \n",
    "    # Build and fit final model with best hyperparameters\n",
    "    forest_model = RandomForestClassifier(random_state=12345, **{k: v for k, v in best_config.items() if k not in ['valid_accuracy','test_accuracy']})\n",
    "    forest_model.fit(features_train, target_train)\n",
    "    \n",
    "    return forest_model, results\n",
    "\n",
    "forest_model, results = explore_random_forest_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")\n",
    "print()\n",
    "\n",
    "evaluate_model(forest_model, features_valid, target_valid, features_test, target_test, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc8562",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98573f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0}\n",
      "Validation Accuracy: 0.7278382581648523\n",
      "Test Accuracy: 0.7589424572317263\n",
      "\n",
      "Configuration: {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0}\n",
      "Validation Accuracy: 0.7293934681181959\n",
      "Test Accuracy: 0.7511664074650077\n",
      "\n",
      "Configuration: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 1.0, 'max_iter': 1000}\n",
      "Validation Accuracy: 0.7262830482115086\n",
      "Test Accuracy: 0.7589424572317263\n",
      "\n",
      "Configuration: {'solver': 'saga', 'penalty': 'l1', 'C': 1.0, 'max_iter': 1000}\n",
      "Validation Accuracy: 0.6936236391912908\n",
      "Test Accuracy: 0.6982892690513219\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dianuselvenbough/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/dianuselvenbough/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'solver': 'saga', 'penalty': 'l2', 'C': 1.0, 'max_iter': 1000}\n",
      "Validation Accuracy: 0.6936236391912908\n",
      "Test Accuracy: 0.6982892690513219\n",
      "\n",
      "Configuration: {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.1}\n",
      "Validation Accuracy: 0.6936236391912908\n",
      "Test Accuracy: 0.6967340590979783\n",
      "\n",
      "Configuration: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
      "Validation Accuracy: 0.7278382581648523\n",
      "Test Accuracy: 0.7558320373250389\n",
      "\n",
      "Configuration: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1, 'max_iter': 1000}\n",
      "Validation Accuracy: 0.7262830482115086\n",
      "Test Accuracy: 0.7589424572317263\n",
      "\n",
      "Configuration: {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10, 'max_iter': 1000}\n",
      "Validation Accuracy: 0.7262830482115086\n",
      "Test Accuracy: 0.7589424572317263\n",
      "\n",
      "\n",
      "Top Performing Configuration:\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0, 'valid_accuracy': 0.7278382581648523, 'test_accuracy': 0.7589424572317263}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 79\u001b[0m\n\u001b[1;32m     73\u001b[0m     best_config \u001b[38;5;241m=\u001b[39m hyperparameter_results[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Declare and fit final Logistic Regression model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     logistic_model \u001b[38;5;241m=\u001b[39m LogisticRegression(\n\u001b[1;32m     77\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12345\u001b[39m,\n\u001b[1;32m     78\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,   \u001b[38;5;66;03m# bump up iterations\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\n\u001b[1;32m     80\u001b[0m )\n\u001b[1;32m     81\u001b[0m     logistic_model\u001b[38;5;241m.\u001b[39mfit(features_train, target_train)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# Evaluate the final model\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Write function to explore logistic regression hyperparameters and evaluate their performance\n",
    "def explore_logistic_regression_hyperparameters(features_train, target_train, features_valid, target_valid, features_test, target_test):\n",
    "    # List of hyperparameter configurations to try\n",
    "    configs = [\n",
    "        # Solvers with default penalty options\n",
    "        {'solver': 'liblinear', 'penalty': 'l1', 'C': 1.0},\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 1.0},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 1.0, 'max_iter': 1000},\n",
    "        {'solver': 'saga', 'penalty': 'l1', 'C': 1.0, 'max_iter': 1000},\n",
    "        {'solver': 'saga', 'penalty': 'l2', 'C': 1.0, 'max_iter': 1000},\n",
    "        \n",
    "        # Regularization strength variations\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.1},\n",
    "        {'solver': 'liblinear', 'penalty': 'l2', 'C': 10},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 0.1, 'max_iter': 1000},\n",
    "        {'solver': 'lbfgs', 'penalty': 'l2', 'C': 10, 'max_iter': 1000},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through configurations\n",
    "    for config in configs:\n",
    "        try:\n",
    "            # Train Logistic Regression\n",
    "            logistic_model = LogisticRegression(\n",
    "                random_state=12345,\n",
    "                **config\n",
    "            )\n",
    "            logistic_model.fit(features_train, target_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            valid_predictions = logistic_model.predict(features_valid)\n",
    "            test_predictions = logistic_model.predict(features_test)\n",
    "            \n",
    "            valid_accuracy = accuracy_score(target_valid, valid_predictions)\n",
    "            test_accuracy = accuracy_score(target_test, test_predictions)\n",
    "            \n",
    "            # Store results\n",
    "            result = config.copy()\n",
    "            result['valid_accuracy'] = valid_accuracy\n",
    "            result['test_accuracy'] = test_accuracy\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Configuration: {config}\")\n",
    "            print(f\"Validation Accuracy: {valid_accuracy}\")\n",
    "            print(f\"Test Accuracy: {test_accuracy}\\n\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Skipping config {config} due to error: {e}\")\n",
    "    \n",
    "    # Sort by test accuracy\n",
    "    results_sorted = sorted(results, key=lambda x: x['test_accuracy'], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop Performing Configuration:\")\n",
    "    if results_sorted:\n",
    "        print(results_sorted[0])\n",
    "    else:\n",
    "        print(\"No valid configuration found.\")\n",
    "    \n",
    "    return results_sorted\n",
    "\n",
    "\n",
    "# --- Use the function ---\n",
    "hyperparameter_results = explore_logistic_regression_hyperparameters(\n",
    "    features_train, target_train,\n",
    "    features_valid, target_valid,\n",
    "    features_test, target_test\n",
    ")\n",
    "\n",
    "# Take the best configuration\n",
    "if hyperparameter_results:\n",
    "    best_config = hyperparameter_results[0]\n",
    "\n",
    "    # Declare and fit final Logistic Regression model\n",
    "    logistic_model = LogisticRegression(\n",
    "        random_state=12345,\n",
    "        **{k: v for k, v in best_config.items() if k not in ['valid_accuracy', 'test_accuracy']}\n",
    "    )\n",
    "    logistic_model.fit(features_train, target_train)\n",
    "\n",
    "    # Evaluate the final model\n",
    "    evaluate_model(logistic_model, features_valid, target_valid, features_test, target_test, model_name=\"Logistic Regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
